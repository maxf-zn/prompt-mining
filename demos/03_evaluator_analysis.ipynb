{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze the performance of baseline evaluators (Llama Guard, Prompt Guard) against ground-truth labels.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The ingestion pipeline can run evaluators on prompts to generate additional labels:\n",
    "- **LlamaGuardEvaluator**: Safety classification using Llama Guard\n",
    "- **PromptGuardEvaluator**: Meta's Prompt Guard 2 for prompt injection detection\n",
    "- **InjecAgentEvaluator**: Detects successful prompt injection attacks\n",
    "\n",
    "This notebook shows how to load these labels and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_mining.classifiers import ClassificationDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Labels from Ingestion Output\n",
    "\n",
    "Use `load_labels()` to get evaluator outputs without loading activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Update this path to point to your ingestion output\n",
    "dataset = ClassificationDataset.from_path(\"/path/to/activations\")\n",
    "\n",
    "# Load only labels (no activations needed)\n",
    "labels_data = dataset.load_labels(filters={'status': 'completed'})\n",
    "print(f\"Loaded {len(labels_data)} completed samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract into DataFrame\n",
    "records = []\n",
    "for item in labels_data:\n",
    "    labels = item['prompt_labels']\n",
    "    records.append({\n",
    "        'run_id': item['run_id'],\n",
    "        'dataset': item['dataset_id'],\n",
    "        'actual_malicious': labels.get('malicious', False),\n",
    "        'lg_label': labels.get('lg_label'),  # Llama Guard output\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nLlama Guard coverage: {df['lg_label'].notna().sum()} / {len(df)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['lg_label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Per-Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to samples with Llama Guard labels\n",
    "df_labeled = df[df['lg_label'].notna()].copy()\n",
    "\n",
    "# Convert to binary: UNSAFE=1 (malicious), SAFE=0 (benign)\n",
    "df_labeled['lg_pred'] = (df_labeled['lg_label'] == 'UNSAFE').astype(int)\n",
    "df_labeled['actual_malicious'] = df_labeled['actual_malicious'].astype(int)\n",
    "\n",
    "# Compute metrics per dataset\n",
    "metrics_list = []\n",
    "for ds in sorted(df_labeled['dataset'].unique()):\n",
    "    subset = df_labeled[df_labeled['dataset'] == ds]\n",
    "    y_true = subset['actual_malicious'].values\n",
    "    y_pred = subset['lg_pred'].values\n",
    "    \n",
    "    n_samples = len(subset)\n",
    "    n_positive = y_true.sum()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Handle single-class datasets\n",
    "    if n_positive == 0 or n_positive == n_samples:\n",
    "        prec, rec, f1 = np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    \n",
    "    metrics_list.append({\n",
    "        'Dataset': ds,\n",
    "        'N': n_samples,\n",
    "        'Pos%': n_positive / n_samples * 100,\n",
    "        'Accuracy': acc * 100,\n",
    "        'Precision': prec * 100 if not np.isnan(prec) else np.nan,\n",
    "        'Recall': rec * 100 if not np.isnan(rec) else np.nan,\n",
    "        'F1': f1 * 100 if not np.isnan(f1) else np.nan,\n",
    "        'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn,\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(\"Per-Dataset Metrics\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_all = df_labeled['actual_malicious'].values\n",
    "y_pred_all = df_labeled['lg_pred'].values\n",
    "\n",
    "print(\"Overall Metrics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df_labeled):,}\")\n",
    "print(f\"Actual positives: {y_true_all.sum():,} ({y_true_all.mean()*100:.1f}%)\")\n",
    "print(f\"Predicted positives: {y_pred_all.sum():,} ({y_pred_all.mean()*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Accuracy:  {accuracy_score(y_true_all, y_pred_all)*100:.1f}%\")\n",
    "print(f\"Precision: {precision_score(y_true_all, y_pred_all)*100:.1f}%\")\n",
    "print(f\"Recall:    {recall_score(y_true_all, y_pred_all)*100:.1f}%\")\n",
    "print(f\"F1 Score:  {f1_score(y_true_all, y_pred_all)*100:.1f}%\")\n",
    "\n",
    "cm = confusion_matrix(y_true_all, y_pred_all, labels=[0, 1])\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"              SAFE    UNSAFE\")\n",
    "print(f\"Actual SAFE   {cm[0,0]:>6,}    {cm[0,1]:>6,}\")\n",
    "print(f\"     UNSAFE   {cm[1,0]:>6,}    {cm[1,1]:>6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detection Rate by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate detection rate for datasets with positives\n",
    "plot_df = metrics_df[metrics_df['Pos%'] > 0].copy()\n",
    "plot_df['Detection_Rate'] = plot_df['TP'] / (plot_df['TP'] + plot_df['FN']) * 100\n",
    "plot_df = plot_df.sort_values('Detection_Rate', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['#e74c3c' if d < 50 else '#f39c12' if d < 80 else '#27ae60' \n",
    "          for d in plot_df['Detection_Rate']]\n",
    "bars = ax.barh(plot_df['Dataset'], plot_df['Detection_Rate'], color=colors)\n",
    "\n",
    "ax.set_xlabel('Detection Rate (Recall) %')\n",
    "ax.set_title('Llama Guard Detection Rate by Dataset')\n",
    "ax.axvline(x=50, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlim(0, 105)\n",
    "\n",
    "for bar, val in zip(bars, plot_df['Detection_Rate']):\n",
    "    ax.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. False Positive Rate on Benign Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPR on all-benign datasets\n",
    "benign_datasets = metrics_df[metrics_df['Pos%'] == 0].copy()\n",
    "benign_datasets['FPR'] = benign_datasets['FP'] / benign_datasets['N'] * 100\n",
    "benign_datasets = benign_datasets.sort_values('FPR', ascending=False)\n",
    "\n",
    "print(\"False Positive Rate on Benign Datasets\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in benign_datasets.iterrows():\n",
    "    print(f\"{row['Dataset']:25s}: {row['FP']:4.0f} / {row['N']:5.0f} = {row['FPR']:.2f}% FPR\")\n",
    "\n",
    "total_fp = benign_datasets['FP'].sum()\n",
    "total_n = benign_datasets['N'].sum()\n",
    "print(f\"\\nOverall FPR on benign data: {total_fp:.0f} / {total_n:.0f} = {total_fp/total_n*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Table with Styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add detection rate column\n",
    "display_df = metrics_df.copy()\n",
    "display_df['Det%'] = display_df.apply(\n",
    "    lambda r: r['TP']/(r['TP']+r['FN'])*100 if (r['TP']+r['FN']) > 0 else np.nan, \n",
    "    axis=1\n",
    ")\n",
    "display_df = display_df.set_index('Dataset')\n",
    "\n",
    "styled = display_df[['N', 'Pos%', 'Accuracy', 'Det%', 'Precision', 'Recall', 'F1']].style\\\n",
    "    .format({\n",
    "        'N': '{:,}',\n",
    "        'Pos%': '{:.1f}%',\n",
    "        'Accuracy': '{:.1f}%',\n",
    "        'Det%': '{:.1f}%',\n",
    "        'Precision': '{:.1f}%',\n",
    "        'Recall': '{:.1f}%',\n",
    "        'F1': '{:.1f}%',\n",
    "    }, na_rep='-')\\\n",
    "    .background_gradient(subset=['Accuracy', 'Det%', 'F1'], cmap='RdYlGn', vmin=0, vmax=100)\n",
    "\n",
    "display(styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading evaluator labels** from ingestion output\n",
    "2. **Computing per-dataset metrics** (accuracy, precision, recall, F1)\n",
    "3. **Analyzing detection rates** across different attack types\n",
    "4. **Measuring false positive rates** on benign data\n",
    "\n",
    "### Key Findings (typical)\n",
    "\n",
    "- **Direct jailbreaks** (advbench, harmbench): High detection rate (~95%+)\n",
    "- **Indirect injection** (bipia, injecagent): Lower detection rate\n",
    "- **Password extraction** (mosscap): Often missed by safety classifiers\n",
    "\n",
    "This analysis helps identify gaps in baseline evaluators that activation-based classifiers may fill."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
